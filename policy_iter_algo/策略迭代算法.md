# 强化学习中值迭代算法的实现
### 重点 
算法实现的重点是理解：值迭代算法和策略迭代算法都是交替进行**value update**和**policy update**，来求解最优策略。 
两个策略的原理具体可以参考文章 
[什么是值迭代和策略迭代算法？](https://zhuanlan.zhihu.com/p/1887821778711201581)

---

### 一、算法通俗讲解
**目标**：通过交替评估当前策略的性能并改进策略，最终找到最优策略（在每个状态下选择长期收益最大的动作）。

---
#### **核心概念**
1. **迭代链路**：$  \pi_{0} { V_{0} \to V_{1} \to V_{2} \dots }\to \pi_{1} \to \pi_{3}\dots $
其中V代表每轮迭代中状态的state value，π代表策略。  大括号里面的代表一个子迭代
2. **策略（Policy）π(a|s)**：表示在状态 `s` 下选择动作 `a` 的概率。  
3. **策略评估**：计算当前策略 `π` 下的状态值函数 `V(s)`。  
4. **策略改进**：基于当前值函数 `V(s)`，更新策略以选择更优动作。  
5. **贝尔曼期望方程**：策略评估的核心公式，用于迭代计算 `V(s)`。

---

#### **算法流程**  
1. **初始化**  
   - 随机初始化策略 `π`（如均匀分布）。  
   - 初始化状态值函数 `V(s) = 0`。

2. **循环更新直至策略稳定**  
   重复以下步骤，直到策略不再变化：  
   - **步骤1：策略评估**  
     迭代计算当前策略 `π` 下的状态值函数 `V(s)`，直至收敛：  
     \[
     V_{\text{new}}(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s,a) \left[ r + \gamma V(s') \right]
     \]  
     更新 `V(s) = V_{\text{new}}(s)`，直到两次迭代的差值小于阈值。  

   - **步骤2：策略改进**  
     对每个状态 `s`，选择使动作值 `q(s,a)` 最大的动作：  
     \[
     \pi_{\text{new}}(a|s) = 
     \begin{cases} 
     1 & \text{if } a = \arg\max_{a} q(s,a) \\
     0 & \text{otherwise}
     \end{cases}
     \]  
     其中，动作值 `q(s,a)` 的计算公式为：  
     \[
     q(s,a) = \sum_{s'} P(s'|s,a) \left[ r + \gamma V(s') \right]
     \]

3. **终止条件**  
   当策略 `π` 在两次迭代中完全相同时，算法终止。

---

#### **与值迭代的区别**  
- **策略迭代**：显式维护策略，分“评估-改进”两步，需多次迭代策略评估，但策略改进更快。  
- **值迭代**：隐式更新策略，直接通过最大化动作值更新 `V(s)`，计算更高效。  

---

#### **伪代码实现**  
```python
def policy_iteration(S, A, P, R, gamma, threshold=1e-6):
    # 初始化策略（均匀分布）
    policy = {s: {a: 1/len(A) for a in A} for s in S}
    V = {s: 0 for s in S}
    policy_stable = False

    while not policy_stable:
        # 策略评估：计算当前策略下的 V(s)
        while True:
            delta = 0
            V_new = {}
            for s in S:
                v = 0
                for a in A:
                    q = sum(P(s, a, s_prime) * (R(s, a, s_prime) + gamma * V[s_prime])
                            for s_prime in S)
                    v += policy[s][a] * q  # 加权求和（策略概率 * 动作值）
                V_new[s] = v
                delta = max(delta, abs(V_new[s] - V[s]))
            V = V_new
            if delta < threshold:
                break

        # 策略改进：贪心更新策略
        policy_stable = True
        for s in S:
            old_action = max(policy[s], key=policy[s].get)  # 原策略选择的动作
            q_values = [sum(P(s, a, s_prime) * (R(s, a, s_prime) + gamma * V[s_prime])
                        for s_prime in S) for a in A]
            best_a = np.argmax(q_values)
            # 更新为确定性策略
            new_policy = {a: 0 for a in A}
            new_policy[best_a] = 1
            if new_policy != policy[s]:
                policy_stable = False
            policy[s] = new_policy

    return V, policy
```

---

#### **关键注意事项**  
1. **策略评估的收敛性**：需多次迭代贝尔曼方程，确保 `V(s)` 精确反映当前策略。  
2. **策略稳定性判断**：若策略不再变化，说明已达到最优。  
3. **计算效率**：策略评估可能耗时较长，可通过动态规划或截断迭代优化。  
4. **折扣因子 γ**：与值迭代相同，控制未来奖励的权重（通常取 0.9~0.99）。  

---
