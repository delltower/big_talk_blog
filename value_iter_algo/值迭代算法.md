# 强化学习中值迭代算法的实现
## 重点 
算法实现的重点是理解：值迭代算法和策略迭代算法都是交替进行value update和policy update，来求解最优策略。 
两个策略的原理具体可以参考文章 
[什么是值迭代和策略迭代算法？](https://zhuanlan.zhihu.com/p/1887821778711201581)
 
## 算法实现
这次我们来实现Value迭代算法 <br>
迭代链路：$ V_{0} \to \pi_{0} \to V_{1}\to \pi_{1}\dots $ <br>
**已知条件** <br>
- p(r|s,a) agent在状态s采取动作a的奖励r的概率，也就是及时奖励分布函数 <br>
- P(s’|s,a) agent在状态s采取动作a之后转移到状态s’的概率，也就是状态转移分布函数 <br>
- 初始化的V <br>
**值迭代算法伪代码：** <br>
![值迭代算法](screen 49.png)
里面每个变量在每轮迭代值都不一样，实际编程需要加上k，实际编程时$ \left \| V_{k+1} - V_{k} \right \| \le \theta $求的是每个元素的差值的最大值。 
## python实现
```python

def value_iteration(grid, theta=1e-4, max_iter = 1000):
    #值迭代算法
    #初始化state 函数
    V = np.zeros((grid.rows, grid.cols))

    for Iter in range(max_iter):
        delta = 0
        new_V = np.copy(V)
        print("---------Iter------------", Iter)
        for i in range(grid.rows):
            for j in range(grid.cols):
                state = (i,j)
                if state in grid.terminal_states:
                    ##终止状态不更新 动作为原地不动
                    new_V[i,j] = 1.0 + grid.gamma*V[i,j]
                    continue
                #计算所有可能动作的值函数
                max_value = -np.inf
                for action in grid.actions:
                    next_state = grid.get_next_state(state, action)
                    reward = grid.get_reward(state, next_state, action)
                    #贝尔曼最优方程更新
                    value = reward + grid.gamma * V[next_state]
                    if value > max_value:
                        max_value = value
                new_V[i,j] = max_value
                delta = max(delta, abs(new_V[i,j]-V[i,j]))

        V = np.copy(new_V)
        print("当前的state value：\n",V)
        iter_policy = extract_policy(grid, V)
        print("当前策略：\n", iter_policy)
        print("debug delta", delta)
        if delta < theta:
            break
    return V

```
value_iteration是我们的值迭代算法的核心代码。 
现在我们已网格世界为例子。网格世界中，agent需要找到到达终点的最优策略。 
首先我们有第一种最简单的网格，就是只有一个终点，并且奖励函数也比较简单，到达终点奖励1，其他状态0.
![最简单的网格](grid_1.png)
那么我们现在开始在这个网格上运行这个算法，看一下state value和policy的变化。
![1](0_value.png)<br>
网格中的箭头，代表agent在这个state找到的最优方向，没有箭头的代表算法现在还没有计算到。我们可以看到刚开始的时候，大部分state value都是0.
<div style="display: flex; gap: 10px;">
  <img src="10_value.png" alt="2" style="width: 24%; height: auto;" />
  <img src="20_value.png" alt="3" style="width: 24%; height: auto;" />
  <img src="30_value.png" alt="4" style="width: 24%; height: auto;" />
</div>
<div style="display: flex; gap: 10px;">
  <img src="40_value.png" alt="5" style="width: 24%; height: auto;" />
  <img src="50_value.png" alt="6" style="width: 24%; height: auto;" />
  <img src="60_value.png" alt="7" style="width: 24%; height: auto;" />
</div>
<div style="display: flex; gap: 10px;">
  <img src="70_value.png" alt="8" style="width: 24%; height: auto;" />
  <img src="80_value.png" alt="9" style="width: 24%; height: auto;" />
  <img src="85_value.png" alt="10" style="width: 24%; height: auto;" />
</div>
在iter=40的时候，delta已经是0.0147，代表两轮算法state value变化已经很小了。 
最终经过88次迭代之后，delta已经到了我们规定的最小值，state value也基本上没有变化了，说明我们已经找到了最优值。下面看一下最优策略和对应的state value。
![11](grid_final.png)<br>