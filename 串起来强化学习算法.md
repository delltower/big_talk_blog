强化学习中的各类算法可以通过其核心思想、发展脉络及相互关系串联起来，形成一个从基础到进阶的完整框架。以下为分层次的梳理：

---

### **1. 价值函数（Value Function）—— 评估策略的基石**
- **定义**：价值函数（状态价值函数 \(V(s)\) 和动作价值函数 \(Q(s, a)\)）是评估在特定状态（或状态-动作对）下，遵循某一策略的期望累积奖励。
- **作用**：为后续算法提供评估标准，是动态规划（DP）和时序差分（TD）方法的核心。

---

### **2. 动态规划（Dynamic Programming）—— 基于模型的优化**
- **适用场景**：已知环境模型（状态转移和奖励函数）。
  - **Value Iteration**：
    - 直接迭代更新状态价值函数 \(V(s)\)，直至收敛后提取最优策略。
    - 特点：效率高，但可能跳过中间策略的显式评估。
  - **Policy Iteration**：
    - 交替进行**策略评估**（计算当前策略的 \(V(s)\)）和**策略改进**（基于 \(V(s)\) 更新策略）。
    - 特点：更稳定，但计算成本较高。

---

### **3. 时序差分（TD Learning）—— 无模型学习的起点**
- **核心思想**：通过与环境交互采样，结合蒙特卡洛（MC）和动态规划（DP），在线更新价值函数。
  - **SARSA（On-Policy）**：
    - 使用当前策略生成的轨迹 \((s, a, r, s', a')\) 更新 \(Q(s, a)\)。
    - 公式：\(Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma Q(s',a') - Q(s,a)]\)。
    - 特点：保守，遵循当前策略（如ε-greedy）。
  - **Q-Learning（Off-Policy）**：
    - 直接学习最优策略的 \(Q(s, a)\)，更新时采用最大Q值。
    - 公式：\(Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]\)。
    - 特点：激进，独立于行为策略（如探索策略）。

---

### **4. 策略梯度（Policy Gradient）—— 直接优化策略**
- **动机**：解决高维/连续动作空间问题，避免基于价值方法的局限性（如最大化偏差）。
- **方法**：参数化策略 \(\pi_\theta(a|s)\)，通过梯度上升最大化期望回报。
  - **REINFORCE**：
    - 使用蒙特卡洛采样估计回报，直接计算策略梯度。
    - 公式：\(\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) G_t\)。
    - 特点：高方差，依赖完整轨迹。
- **优势**：适用于复杂策略，但需处理高方差问题。

---

### **5. Actor-Critic —— 价值与策略的结合**
- **核心思想**：融合策略梯度（Actor）和价值函数（Critic），Critic提供低方差估计指导Actor更新。
  - **Actor**：调整策略参数 \(\theta\)，负责动作选择。
  - **Critic**：评估状态价值 \(V(s)\) 或优势函数 \(A(s,a)\)，提供反馈。
  - **算法**：
    - **A2C（Advantage Actor-Critic）**：用优势函数 \(A(s,a) = Q(s,a) - V(s)\) 替代原始回报，降低方差。
    - **A3C（Asynchronous）**：并行多个Actor-Critic，加速训练。
- **优势**：比纯策略梯度更稳定，比纯价值方法更灵活。

---

### **6. 总结脉络：算法演进与关系**
1. **基础工具**：价值函数（\(V(s)\) 和 \(Q(s,a)\)）为所有方法提供评估基准。
2. **已知模型优化**：动态规划（Value/Policy Iteration）解决理想化问题。
3. **无模型学习**：TD方法（SARSA/Q-Learning）通过采样逼近价值函数。
4. **策略直接优化**：Policy Gradient绕过价值函数，处理复杂动作空间。
5. **融合与平衡**：Actor-Critic结合价值评估与策略优化，兼顾稳定性和灵活性。

---

### **关系图**
```
Value Function
│
├── Dynamic Programming → Value Iteration
│                └── Policy Iteration
│
├── TD Learning → SARSA (On-Policy)
│           └── Q-Learning (Off-Policy)
│
└── Policy Gradient → Actor-Critic (A2C/A3C)
```

通过这一框架，不同算法在解决问题时互补：从模型驱动的动态规划，到无模型的TD学习，再到直接策略优化与混合方法，逐步覆盖更复杂的现实场景。